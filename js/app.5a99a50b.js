(function(){"use strict";var t={3885:function(t,e,a){var s=a(4720),i=a.n(s),n=a(144),o=function(){var t=this,e=t._self._c;return e("div",{attrs:{id:"app"}},[e("router-view")],1)},l=[],r=a(1001),c={},d=(0,r.Z)(c,o,l,!1,null,null,null),u=d.exports,p=a(8345),v=function(){var t=this,e=t._self._c;return e("div",{staticClass:"HomePage h-full w-full text-lg"},[e("DropMenu"),e("TopMenu",{ref:"TopMenu"}),e("HomePage"),e("ExperimentsPage",{ref:"experiment"}),e("BibtexView"),e("FooterPart")],1)},h=[],f=function(){var t=this,e=t._self._c;return e("div",{staticClass:"DropMenu w-full h-full flex justify-center pt-5"},[e("el-dropdown",[e("span",{staticClass:"el-dropdown-link text-center"},[t._v(" More Research"),e("i",{staticClass:"el-icon-arrow-down el-icon--right"})]),e("el-dropdown-menu",{attrs:{slot:"dropdown"},slot:"dropdown"},[e("el-dropdown-item",[t._v("Wait...")])],1)],1)],1)},m=[],b={},g=b,_=(0,r.Z)(g,f,m,!1,null,null,null),w=_.exports,C=function(){var t=this,e=t._self._c;return e("div",{staticClass:"TopMenu h-full text-center flex justify-center items-center pt-10 flex-col"},[t._m(0),e("div",{staticClass:"pt-5"},[t._v(" A large-scale, high-quality, comprehensive and specialized database for pathology ")]),t._m(1),t._m(2),t._m(3),e("div",{staticClass:"btn_list pt-5 flex items-center text-base pb-8"},[e("div",{staticClass:"cursor-pointer rounded-3xl text-white pl-3 pr-3 pt-2 pb-2",staticStyle:{background:"#363636"}},[t._v(" arXiv ")]),e("div",{staticClass:"cursor-pointer rounded-3xl text-white pl-3 pr-3 pt-2 pb-2",staticStyle:{background:"#363636"}},[t._v(" Dataset ")]),e("div",{staticClass:"cursor-pointer rounded-3xl text-white pl-3 pr-3 pt-2 pb-2",staticStyle:{background:"#363636"}},[t._v(" Code ")]),e("div",{staticClass:"cursor-pointer rounded-3xl text-white pl-3 pr-3 pt-2 pb-2",staticStyle:{background:"#363636"},on:{click:t.toLeaderBoard}},[t._v(" Leaderboard ")]),e("div",{staticClass:"cursor-pointer rounded-3xl text-white pl-3 pr-3 pt-2 pb-2",staticStyle:{background:"#363636"}},[t._v(" Twitter ")]),e("div",{staticClass:"cursor-pointer rounded-3xl text-white pl-3 pr-3 pt-2 pb-2",staticStyle:{background:"#363636"}},[t._v(" Examples ")])]),t._m(4),t._m(5)])},x=[function(){var t=this,e=t._self._c;return e("div",{staticClass:"Title"},[t._v(" PathMMU: A Massive Multimodal Expert-Level Benchmark "),e("br"),e("div",{staticClass:"pt-2"}),t._v(" for Understanding and Reasoning in Pathology ")])},function(){var t=this,e=t._self._c;return e("div",{staticClass:"author pt-5"},[t._v(" Yuxuan Sun,Hao Wu,Chenglu Zhu, "),e("br"),t._v(" Sunyi Zheng,Qizi Chen,Kai Zhang,Yunlong Zhang,XXXX,Mengyue Zheng, "),e("br"),t._v(" Jingxiong Li, Xinheng Lyu, Tao Lin,Lin Yang ")])},function(){var t=this,e=t._self._c;return e("div",{staticClass:"author pt-10"},[t._v(" Westlake University, Macau University of Science and Technology, Jiangnan University,"),e("br"),t._v(" The Ohio State University, Zhejiang University, ")])},function(){var t=this,e=t._self._c;return e("div",{staticClass:"pt-5"},[e("div",[t._v("*Core Contributors")]),e("div",[t._v(" †Corresponding to: "),e("a",{staticClass:"text-blue-400",attrs:{href:""}},[t._v("xiangyue.work@gmail.com")]),t._v(", "),e("a",{staticClass:"text-blue-400",attrs:{href:""}},[t._v("su.809@osu.edu, wenhuchen@uwaterloo.ca")])])])},function(){var t=this,e=t._self._c;return e("div",{staticClass:"overall w-7/12 pt-5"},[e("img",{attrs:{src:"resources/img/figures/overall.png",alt:""}}),e("div",{staticClass:"pt-10 pb-3 text-justify"},[t._v(" The emergence of large multimodal models has unlocked remarkable potential in AI, particularly in pathology. However, the lack of specialized, high-quality benchmark impeded their development and precise evaluation. To address this, we introduce PathMMU, the largest and highest-quality expert-validated pathology benchmark for LMMs. It comprises 33,573 multimodal multi-choice questions and 21,599 images from various sources, and an explanation for the correct answer accompanies each question. The construction of PathMMU capitalizes on the robust capabilities of GPT-4V, utilizing approximately 30,000 gathered image-caption pairs to generate Q&As. Significantly, to maximize PathMMU's authority, we invite six pathologists to scrutinize each question under strict standards in PathMMU's validation and test sets, while simultaneously setting an expert-level performance benchmark for PathMMU. We conduct extensive evaluations, including zero-shot assessments of 14 open-sourced and 3 closed-sourced LMMs and their robustness to image corruption. We also fine-tune representative LMMs to assess their adaptability to PathMMU. The empirical findings indicate that advanced LMMs struggle with the challenging PathMMU benchmark, with the top-performing LMM, GPT-4V, achieving only a 51.7% zero-shot performance, significantly lower than the 71.4% demonstrated by human pathologists. After fine-tuning, even open-sourced LMMs can surpass GPT-4V with a performance of over 60%, but still fall short of the expertise shown by pathologists. ")])])},function(){var t=this,e=t._self._c;return e("div",{staticClass:"Intro pt-10 w-7/12"},[e("div",{staticClass:"head font-bold pb-5"},[t._v("Introduction")]),e("div",{staticClass:"content text-justify pb-10"},[t._v(" The Pathology Multimodal Multitask Unsupervised Benchmark (MMMU) dataset is a large-scale, multimodal, multitask dataset for understanding and reasoning in pathology. The dataset is designed to be comprehensive, diverse, and challenging, and is intended to serve as a benchmark for research in multimodal pathology understanding and reasoning. ")])])}],y={data(){return{publicUrl:"./public/",topMenu:[{content:"HOME",id:0},{content:"ABOUT",id:2},{content:"CONCACT",id:3}]}},methods:{toLeaderBoard(){const t=this.$parent.$refs.experiment.$refs.leaderboard;t&&t.scrollIntoView({behavior:"smooth"})}}},k=y,M=(0,r.Z)(k,C,x,!1,null,"6d789d5e",null),T=M.exports,S=function(){var t=this,e=t._self._c;return e("div",{staticClass:"HomePage w-full h-full"},[e("div",{staticClass:"bg w-full h-20 flex justify-center items-center font-bold text-4xl"},[t._v(" PATHMMU BENCHMARK ")]),e("div",{staticClass:"PageContent flex justify-center items-center flex-col"},[e("div",{staticClass:"headtitle text-3xl font-bold p-5"},[t._v("Overview")]),e("div",{staticClass:"overview_content w-7/12"},[e("div",{staticClass:"text-justify"},[t._v(" 占位符占位符占位符占位符占位符占位符占位符占位符占位符占位符占位符占位符占位符占位符占位符占位符占位符占位符占位符占位符占位符占位符v占位符占位符占位符占位符占位符占位符占位符占位符占位符占位符占位符占位符占位符占位符占位符占位符占位符占位符占位符占位符占位符占位符v占位符占位符占位符占位符占位符占位符占位符占位符占位符占位符占位符占位符占位符占位符占位符占位符占位符占位符占位符占位符占位符占位符v占位符占位符占位符占位符占位符占位符占位符占位符占位符占位符占位符占位符占位符占位符占位符占位符占位符占位符占位符占位符占位符占位符v占位符占位符占位符占位符占位符占位符占位符占位符占位符占位符占位符占位符占位符占位符占位符占位符占位符占位符占位符占位符占位符占位符v占位符占位符占位符占位符占位符占位符占位符占位符占位符占位符占位符占位符占位符占位符占位符占位符占位符占位符占位符占位符占位符占位符v ")]),e("el-empty",{attrs:{"image-size":400}}),e("div")],1)]),t._m(0),t._m(1),t._m(2),t._m(3),t._m(4)])},L=[function(){var t=this,e=t._self._c;return e("div",{staticClass:"w-7/12 pt-10 pb-5",staticStyle:{margin:"0 auto"}},[e("div",{staticClass:"FeatureTitle"},[t._v("Key Features")]),e("div",{staticClass:"FeatureContent grid grid-cols-3 gap-3"},[e("div",[e("div",{staticClass:"SubTitle"},[t._v("Large-Scale")]),e("div",{staticClass:"subContent text-left"},[t._v(" The dataset contains more than 10,000 video segments of real-world moving objects and over 1.5 million manually labeled bounding boxes ")])]),e("div",[e("div",{staticClass:"SubTitle"},[t._v("Large-Scale")]),e("div",{staticClass:"subContent text-left"},[t._v(" The dataset contains more than 10,000 video segments of real-world moving objects and over 1.5 million manually labeled bounding boxes ")])]),e("div",[e("div",{staticClass:"SubTitle"},[t._v("Large-Scale")]),e("div",{staticClass:"subContent text-left"},[t._v(" The dataset contains more than 10,000 video segments of real-world moving objects and over 1.5 million manually labeled bounding boxes ")])]),e("div",[e("div",{staticClass:"SubTitle"},[t._v("Large-Scale")]),e("div",{staticClass:"subContent text-left"},[t._v(" The dataset contains more than 10,000 video segments of real-world moving objects and over 1.5 million manually labeled bounding boxes ")])]),e("div",[e("div",{staticClass:"SubTitle"},[t._v("Large-Scale")]),e("div",{staticClass:"subContent text-left"},[t._v(" The dataset contains more than 10,000 video segments of real-world moving objects and over 1.5 million manually labeled bounding boxes ")])]),e("div",[e("div",{staticClass:"SubTitle"},[t._v("Large-Scale")]),e("div",{staticClass:"subContent text-left"},[t._v(" The dataset contains more than 10,000 video segments of real-world moving objects and over 1.5 million manually labeled bounding boxes ")])])])])},function(){var t=this,e=t._self._c;return e("div",{staticClass:"w-7/12 pt-10 pb-5",staticStyle:{margin:"0 auto"}},[e("div",{staticClass:"font-bold text-red-500 text-2xl pb-3"},[t._v("Latest News")]),e("ul",[e("li",[t._v(" [2023.09.12] The SOTVerse paper has been accepted by International Journal of Computer Vision (IJCV)! ")]),e("li",[t._v(" [2022.04.18] We have released SOTVerse, a user-defined task space of single object tracking, and the related paper has been released on arXiv. ")]),e("li",[t._v(' [2022.03.29] The IEEE TPAMI paper is selected as "ESI Hot Papers"! ')]),e("li",[t._v(' [2021.09.14] The IEEE TPAMI paper is selected as "ESI Highly Cited Papers"! ')])])])},function(){var t=this,e=t._self._c;return e("div",{staticClass:"w-7/12 pt-10 pb-5",staticStyle:{margin:"0 auto"}},[e("div",{staticClass:"text-2xl pb-3"},[t._v("Publications")]),e("div",{staticClass:"flex items-center"},[e("img",{attrs:{src:"resources/img/figures/publication.png",alt:""}}),e("div",{staticClass:"pl-5"},[e("div",{staticClass:"intro"},[t._v(" GOT-10k: A Large High-Diversity Benchmark for Generic Object Tracking in the Wild. L. Huang*, X. Zhao*, and K. Huang. ( *Equal contribution) IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI). ")]),e("div",{staticClass:"link"},[e("a",{staticClass:"text-blue-600",attrs:{href:""}},[t._v("[PDF] ")]),e("a",{staticClass:"text-blue-600",attrs:{href:""}},[t._v("[ArXiv]")]),e("a",{staticClass:"text-blue-600",attrs:{href:""}},[t._v("[BibTex]")])])])]),e("div",{staticClass:"text-gray-500 pt-5"},[e("i",[t._v("Please cite this paper if GOT-10k helps your research.")])])])},function(){var t=this,e=t._self._c;return e("div",{staticClass:"PageContent flex justify-center items-center flex-col pt-10"},[e("div",{staticClass:"headtitle text-3xl font-bold p-5"},[t._v(" Comparisons with Existing Benchmarks ")]),e("div",{staticClass:"overview_content w-7/12"},[e("div",{staticClass:"text-justify"},[t._v(" We introduce the Massive Multi-discipline Multimodal Understanding and Reasoning (MMMU) benchmark, a novel benchmark meticulously curated to assess the expert-level multimodal understanding capability of foundation models across a broad scope of tasks. Covering subjects across disciplines, including Art, Business, Health & Medicine, Science, Humanities & Social Science, and Tech & Engineering, and over subfields. The detailed subject coverage and statistics are detailed in the figure. The questions in our benchmark were manually collected by a team of college students (including coauthors) from various disciplines and subjects, drawing from online sources, textbooks, and lecture materials. ")]),e("img",{staticClass:"pt-10 pb-2",attrs:{src:"resources/img/figures/data_comparison.png",alt:""}}),e("div",{staticClass:"pb-10 pt-5"},[t._v(" Sampled MMMU examples from each discipline. The questions and images need expert-level knowledge to understand and reason. ")])])])},function(){var t=this,e=t._self._c;return e("div",{staticClass:"w-7/12 pb-20",staticStyle:{margin:"0 auto"}},[e("div",{staticClass:"text-3xl font-bold p-5 text-center"},[t._v("Methodology")]),e("div",{staticClass:"flex items-center"},[e("img",{attrs:{src:"resources/img/figures/question_generation.png",alt:""}})])])}],P={},j=P,O=(0,r.Z)(j,S,L,!1,null,"0673513f",null),B=O.exports,I=function(){var t=this,e=t._self._c;return e("div",{staticClass:"Experiments w-full h-3/5"},[e("div",{staticClass:"bg w-full h-20 flex justify-center items-center font-bold text-4xl"},[t._v(" Experiment Results ")]),e("div",{staticClass:"flex justify-center items-center flex-col"},[e("div",{ref:"leaderboard",staticClass:"ExTitle text-3xl font-bold p-5 pt-10",attrs:{id:"Leaderboard"}},[t._v(" Leaderboard ")]),e("div",{staticClass:"leadContent w-7/12 text-justify"},[t._v(" We evaluate various models including LLMs and LMMs. In each type, we consider both closed- and open-source models. Our evaluation is conducted under a zero-shot setting to assess the capability of models to generate accurate answers without fine-tuning or few-shot demonstrations on our benchmark. For all models, we use the default prompt provided by each model for multi-choice or open QA, if available. If models do not provide prompts for task types in MMMU, we conduct prompt engineering on the validation set and use the most effective prompt for the later zero-shot experiment. ")])]),e("div",{staticClass:"w-full h-full flex justify-center items-center scale-75 relative bottom-12"},[e("div",{staticClass:"main",staticStyle:{width:"95rem"}},[e("div",{staticClass:"table"},[e("ul",{staticStyle:{"padding-top":"20px",background:"rgb(221, 221, 221)"}},[e("li",{staticClass:"one"}),t._l(t.titles,(function(a,s){return e("li",{key:s,staticClass:"two"},[0==s?e("div",[e("P",[t._v("Validation")]),e("P",[t._v("Overall")])],1):e("div",[e("p",[t._v(t._s(a))])])])}))],2),e("ul",{staticStyle:{"padding-bottom":"10px",background:"rgb(221, 221, 221)"}},[e("li",{staticClass:"one"}),t._l(t.titles,(function(a,s){return e("li",{key:s,staticClass:"two"},[e("div",0==s?[t._m(0,!0)]:[e("div",{staticClass:"name"},[e("div",{staticClass:"details"},[e("p",[t._v("Tiny")]),e("p",{staticClass:"text-xs",staticStyle:{"padding-top":"5px"}},[t._v(" ( "+t._s(t.details[s].t1)+" ) ")])]),e("div",{staticClass:"details"},[e("p",[t._v("All")]),e("p",{staticClass:"text-xs",staticStyle:{"padding-top":"5px"}},[t._v(" ( "+t._s(t.details[s].t2)+" ) ")])])])])])}))],2),e("div",{staticClass:"table-2",staticStyle:{background:"rgb(241, 250, 251)"}},t._l(t.table1,(function(a){return e("ul",{key:a.prop,staticClass:"row-1",attrs:{st:""}},[e("li",{staticClass:"one pl-2"},[e("a",{staticClass:"text-blue-700",attrs:{href:a.link,target:"_blank"}},[t._v(t._s(a.prop))])]),t._l(a.data,(function(s,i){return e("li",{key:i,staticClass:"two"},[e("div",0==i?[e("div",{staticClass:"details"},[e("p",[t._v(t._s(a.sorce))])])]:[e("div",{staticClass:"name"},[e("div",{staticClass:"details"},[e("p",[t._v(t._s(a.data[i].t1))])]),e("div",{staticClass:"details"},[e("p",[t._v(t._s(a.data[i].t2))])])])])])}))],2)})),0),e("div",{staticClass:"title-2"},[t._v(" Large Multimodal Models (LMMs): Text + Image as Input ")]),e("div",{staticClass:"table-2",staticStyle:{background:"rgb(249, 242, 248)"}},t._l(t.table2,(function(a){return e("ul",{key:a.prop,staticClass:"row-1"},[e("li",{staticClass:"one pl-2"},[e("a",{staticClass:"text-blue-700",attrs:{href:a.link,target:"_blank"}},[t._v(t._s(a.prop))])]),t._l(a.data,(function(s,i){return e("li",{key:i,staticClass:"two"},[e("div",0==i?[e("div",{staticClass:"details"},[e("p",[t._v(t._s(a.sorce))])])]:[e("div",{staticClass:"name"},[e("div",{staticClass:"details"},[e("p",[t._v(t._s(a.data[i].t1))])]),e("div",{staticClass:"details"},[e("p",[t._v(t._s(a.data[i].t2))])])])])])}))],2)})),0),e("div",{staticClass:"title-2"},[t._v(" Large Language Models (LLMs): Only Text as Input ")]),e("div",{staticClass:"table-2",staticStyle:{background:"rgb(241, 250, 251)"}},t._l(t.table3,(function(a){return e("ul",{key:a.prop,staticClass:"row-1"},[e("li",{staticClass:"one pl-2"},[e("a",{staticClass:"text-blue-700",attrs:{href:a.link,target:"_blank"}},[t._v(t._s(a.prop))])]),t._l(a.data,(function(s,i){return e("li",{key:i,staticClass:"two"},[e("div",0==i?[e("div",{staticClass:"details"},[e("p",[t._v(t._s(a.sorce))])])]:[e("div",{staticClass:"name"},[e("div",{staticClass:"details"},[e("p",[t._v(t._s(a.data[i].t1))])]),e("div",{staticClass:"details"},[e("p",[t._v(t._s(a.data[i].t2))])])])])])}))],2)})),0)])])])])},E=[function(){var t=this,e=t._self._c;return e("div",{staticClass:"details"},[e("p",[t._v("-")]),e("p",{staticClass:"text-xs",staticStyle:{"padding-top":"5px"}},[t._v("( 666 )")])])}],A={data(){return{headers:["Validation Overall","Test Overall","WebPathology","Twitter","Youtube","Book"],tableData:[["Random Choice",25.1,25.8,25.2,24.7,25.9,25.5],["Frequent Choice",29.1,27.5,25.8,27.7,30.2,28.4]],titles:["Validation","Test Overall","WebPathology","Twitter","Youtube","Book"],details:[{t1:3445,t2:4667},{t1:3445,t2:4667},{t1:3445,t2:4667},{t1:3445,t2:4667},{t1:3445,t2:4667},{t1:3445,t2:4667}],table1:[{prop:"Random Choice",sorce:"34.1",link:"https://www.baidu.com/",data:[{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4}]},{prop:"Frequent Choice",sorce:"34.1",link:"https://www.baidu.com/",data:[{t1:25.1,t2:29.4},{t1:22.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4}]},{prop:"Expert performance",link:"https://www.baidu.com/",sorce:"34.1",data:[{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:23.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4}]}],table2:[{prop:"OpenFlamingo2-9B",link:"https://www.baidu.com/",sorce:"34.1",data:[{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4}]},{prop:"Kosmos2",link:"https://www.baidu.com/",sorce:"34.1",data:[{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4}]},{prop:"Fuyu-8B",link:"https://www.baidu.com/",sorce:"34.1",data:[{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4}]},{prop:"MiniGPT4-Vicuna-13B",link:"https://www.baidu.com/",sorce:"34.1",data:[{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4}]},{prop:"LLaMA-Adapter2-7B",link:"https://www.baidu.com/",sorce:"34.1",data:[{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4}]},{prop:"Otter",sorce:"34.1",link:"https://www.baidu.com/",data:[{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4}]},{prop:"CogVLM",sorce:"34.1",link:"https://www.baidu.com/",data:[{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4}]},{prop:"BLIP-2 FLAN-T5-XL",sorce:"34.1",link:"https://www.baidu.com/",data:[{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4}]},{prop:"BLIP-2 FLAN-T5-XXL",sorce:"34.1",link:"https://www.baidu.com/",data:[{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4}]},{prop:"Owen-VL-7B",sorce:"34.1",link:"https://www.baidu.com/",data:[{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4}]},{prop:"LLaVA-1.5-7B",sorce:"34.1",link:"https://www.baidu.com/",data:[{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4}]},{prop:"LLaVA-1.5-13B",sorce:"34.1",link:"https://www.baidu.com/",data:[{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4}]},{prop:"InstructBLIP-T5-XL",sorce:"34.1",link:"https://www.baidu.com/",data:[{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4}]},{prop:"InstructBLIP-TS-XXL",sorce:"34.1",link:"https://www.baidu.com/",data:[{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4}]},{prop:"Qwen-VL-PLUS",sorce:"34.1",link:"https://www.baidu.com/",data:[{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4}]},{prop:"Gemini Pro Vision",sorce:"34.1",link:"https://www.baidu.com/",data:[{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4}]},{prop:"GPT-4V",sorce:"34.1",link:"https://www.baidu.com/",data:[{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4}]}],table3:[{prop:" Vicuna-v1.5-13B",sorce:"34.1",link:"https://www.baidu.com/",data:[{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4}]},{prop:"GPT-4 Turbo",sorce:"34.1",link:"https://www.baidu.com/",data:[{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4}]},{prop:"GPT-3.5 Turbo",sorce:"34.1",link:"https://www.baidu.com/",data:[{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4}]},{prop:"Gemini Pro",sorce:"34.1",link:"https://www.baidu.com/",data:[{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4}]},{prop:"ERNIE-Bot 4.0",sorce:"34.1",link:"https://www.baidu.com/",data:[{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4}]}]}}},U=A,V=(0,r.Z)(U,I,E,!1,null,"1104c062",null),Z=V.exports,H=function(){var t=this;t._self._c;return t._m(0)},X=[function(){var t=this,e=t._self._c;return e("div",{staticClass:"FooterPart w-full h-48 flex justify-center items-center text-base mt-5",staticStyle:{"background-color":"#fafafa"}},[e("p",[t._v(" The copyright belongs to the team ")])])}],F={},W=F,G=(0,r.Z)(W,H,X,!1,null,null,null),Y=G.exports,q=function(){var t=this;t._self._c;return t._m(0)},z=[function(){var t=this,e=t._self._c;return e("div",{staticClass:"BibtexView w-full h-full flex justify-center items-center flex-col relative bottom-10"},[e("div",{staticClass:"text-3xl font-bold p-5"},[t._v("BibTeX")]),e("div",{staticClass:"Code w-7/12"},[e("div",{staticClass:"content text-gray-500 text-sm"},[t._v(" @article{yue2023mmmu, "),e("br"),t._v("      title={MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI},"),e("br"),t._v("      author={Xiang Yue and Yuansheng Ni and Kai Zhang and Tianyu Zheng and Ruoqi Liu and Ge Zhang and Samuel Stevens and Dongfu Jiang and Weiming Ren and Yuxuan Sun and Cong Wei and Botao Yu and Ruibin Yuan and Renliang Sun and Ming Yin and Boyuan Zheng and Zhenzhu Yang and Yibo Liu and Wenhao Huang and Huan Sun and Yu Su and Wenhu Chen}, "),e("br"),t._v("      journal={arXiv preprint arXiv:2311.16502},"),e("br"),t._v("      year={2023}, "),e("br"),t._v("   } ")])])])}],R={},D=R,N=(0,r.Z)(D,q,z,!1,null,"33157e76",null),K=N.exports,J={name:"HomeView",components:{TopMenu:T,HomePage:B,ExperimentsPage:Z,DropMenu:w,FooterPart:Y,BibtexView:K},data(){return{}},mounted(){},methods:{}},Q=J,$=(0,r.Z)(Q,v,h,!1,null,null,null),tt=$.exports;n["default"].use(p.ZP);const et=[{path:"/",name:"home",component:tt}],at=new p.ZP({mode:"hash",base:"",routes:et});var st=at;n["default"].use(i()),n["default"].config.productionTip=!1,new n["default"]({router:st,render:function(t){return t(u)}}).$mount("#app")}},e={};function a(s){var i=e[s];if(void 0!==i)return i.exports;var n=e[s]={id:s,loaded:!1,exports:{}};return t[s](n,n.exports,a),n.loaded=!0,n.exports}a.m=t,function(){a.amdO={}}(),function(){var t=[];a.O=function(e,s,i,n){if(!s){var o=1/0;for(d=0;d<t.length;d++){s=t[d][0],i=t[d][1],n=t[d][2];for(var l=!0,r=0;r<s.length;r++)(!1&n||o>=n)&&Object.keys(a.O).every((function(t){return a.O[t](s[r])}))?s.splice(r--,1):(l=!1,n<o&&(o=n));if(l){t.splice(d--,1);var c=i();void 0!==c&&(e=c)}}return e}n=n||0;for(var d=t.length;d>0&&t[d-1][2]>n;d--)t[d]=t[d-1];t[d]=[s,i,n]}}(),function(){a.n=function(t){var e=t&&t.__esModule?function(){return t["default"]}:function(){return t};return a.d(e,{a:e}),e}}(),function(){a.d=function(t,e){for(var s in e)a.o(e,s)&&!a.o(t,s)&&Object.defineProperty(t,s,{enumerable:!0,get:e[s]})}}(),function(){a.g=function(){if("object"===typeof globalThis)return globalThis;try{return this||new Function("return this")()}catch(t){if("object"===typeof window)return window}}()}(),function(){a.o=function(t,e){return Object.prototype.hasOwnProperty.call(t,e)}}(),function(){a.r=function(t){"undefined"!==typeof Symbol&&Symbol.toStringTag&&Object.defineProperty(t,Symbol.toStringTag,{value:"Module"}),Object.defineProperty(t,"__esModule",{value:!0})}}(),function(){a.nmd=function(t){return t.paths=[],t.children||(t.children=[]),t}}(),function(){var t={143:0};a.O.j=function(e){return 0===t[e]};var e=function(e,s){var i,n,o=s[0],l=s[1],r=s[2],c=0;if(o.some((function(e){return 0!==t[e]}))){for(i in l)a.o(l,i)&&(a.m[i]=l[i]);if(r)var d=r(a)}for(e&&e(s);c<o.length;c++)n=o[c],a.o(t,n)&&t[n]&&t[n][0](),t[n]=0;return a.O(d)},s=self["webpackChunkportfolio"]=self["webpackChunkportfolio"]||[];s.forEach(e.bind(null,0)),s.push=e.bind(null,s.push.bind(s))}();var s=a.O(void 0,[998],(function(){return a(3885)}));s=a.O(s)})();
//# sourceMappingURL=app.5a99a50b.js.map