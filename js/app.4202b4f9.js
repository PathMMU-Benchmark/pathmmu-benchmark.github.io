(function(){"use strict";var t={6255:function(t,e,a){var s=a(4720),i=a.n(s),n=a(144),o=function(){var t=this,e=t._self._c;return e("div",{attrs:{id:"app"}},[e("router-view")],1)},l=[],r=a(1001),c={},d=(0,r.Z)(c,o,l,!1,null,null,null),u=d.exports,p=a(8345),v=function(){var t=this,e=t._self._c;return e("div",{staticClass:"HomePage h-full w-full text-lg"},[e("TopMenu",{ref:"TopMenu"}),e("HomePage"),e("ExperimentsPage"),e("ThirdPart"),e("FooterPart")],1)},f=[],m=function(){var t=this;t._self._c;return t._m(0)},h=[function(){var t=this,e=t._self._c;return e("div",{staticClass:"TopMenu h-full text-center flex justify-center items-center pt-40 flex-col"},[e("div",{staticClass:"Title"},[t._v(" PathMMU: A Massive Multimodal Expert-Level Benchmark for Understanding "),e("br"),t._v(" and Reasoning in Pathology ")]),e("div",{staticClass:"author pt-10 font-bold"},[t._v("Author Name")]),e("div",{staticClass:"pt-1"},[t._v("Affiliation")]),e("div",[t._v("email@example.com")]),e("div",{staticClass:"overall w-7/12 pt-5"},[e("img",{attrs:{src:"resources/img/figures/overall.png",alt:""}}),e("div",{staticClass:"pt-3 pb-3"},[t._v(" Figure 1: An overview of the PathMMU benchmark: PathMMU is constructed using a diverse range of rich data sources. It comprises expert-level, multimodal, multi-choice questions in pathology, collaboratively crafted by AI and human pathology experts. Notably, even the most advanced Large Multimodal Models substantially underperform when benchmarked against human experts on the PathMMU. ")])]),e("div",{staticClass:"Intro pt-10 w-7/12"},[e("div",{staticClass:"head font-bold pb-5"},[t._v("Introduction")]),e("div",{staticClass:"content text-center pb-10"},[t._v(" The Pathology Multimodal Multitask Unsupervised Benchmark (MMMU) dataset is a large-scale, multimodal, multitask dataset for understanding and reasoning in pathology. The dataset is designed to be comprehensive, diverse, and challenging, and is intended to serve as a benchmark for research in multimodal pathology understanding and reasoning. ")])])])}],b={data(){return{publicUrl:"./public/",topMenu:[{content:"HOME",id:0},{content:"ABOUT",id:2},{content:"CONCACT",id:3}]}}},g=b,w=(0,r.Z)(g,m,h,!1,null,"22445a9a",null),_=w.exports,C=function(){var t=this;t._self._c;return t._m(0)},x=[function(){var t=this,e=t._self._c;return e("div",{staticClass:"HomePage w-full h-full"},[e("div",{staticClass:"bg w-full h-20 flex justify-center items-center font-bold text-4xl"},[t._v(" PATHMMU BENCHMARK ")]),e("div",{staticClass:"PageContent flex justify-center items-center flex-col"},[e("div",{staticClass:"headtitle text-3xl font-bold p-5"},[t._v("Overview")]),e("div",{staticClass:"overview_content w-7/12"},[e("div",[t._v(" We introduce the Massive Multi-discipline Multimodal Understanding and Reasoning (MMMU) benchmark, a novel benchmark meticulously curated to assess the expert-level multimodal understanding capability of foundation models across a broad scope of tasks. Covering subjects across disciplines, including Art, Business, Health & Medicine, Science, Humanities & Social Science, and Tech & Engineering, and over subfields. The detailed subject coverage and statistics are detailed in the figure. The questions in our benchmark were manually collected by a team of college students (including coauthors) from various disciplines and subjects, drawing from online sources, textbooks, and lecture materials. ")]),e("img",{staticClass:"pt-10 pb-10",attrs:{src:"resources/img/figures/overall.png",alt:""}}),e("div")])]),e("div",{staticClass:"w-7/12 pt-10",staticStyle:{margin:"0 auto"}},[e("div",{staticClass:"FeatureTitle"},[t._v("Key Features")]),e("div",{staticClass:"FeatureContent grid grid-cols-3 gap-3"},[e("div",[e("div",{staticClass:"SubTitle"},[t._v("Large-Scale")]),e("div",{staticClass:"subContent"},[t._v(" The dataset contains more than 10,000 video segments of real-world moving objects and over 1.5 million manually labeled bounding boxes ")])]),e("div",[e("div",{staticClass:"SubTitle"},[t._v("Large-Scale")]),e("div",{staticClass:"subContent"},[t._v(" The dataset contains more than 10,000 video segments of real-world moving objects and over 1.5 million manually labeled bounding boxes ")])]),e("div",[e("div",{staticClass:"SubTitle"},[t._v("Large-Scale")]),e("div",{staticClass:"subContent"},[t._v(" The dataset contains more than 10,000 video segments of real-world moving objects and over 1.5 million manually labeled bounding boxes ")])]),e("div",[e("div",{staticClass:"SubTitle"},[t._v("Large-Scale")]),e("div",{staticClass:"subContent"},[t._v(" The dataset contains more than 10,000 video segments of real-world moving objects and over 1.5 million manually labeled bounding boxes ")])]),e("div",[e("div",{staticClass:"SubTitle"},[t._v("Large-Scale")]),e("div",{staticClass:"subContent"},[t._v(" The dataset contains more than 10,000 video segments of real-world moving objects and over 1.5 million manually labeled bounding boxes ")])]),e("div",[e("div",{staticClass:"SubTitle"},[t._v("Large-Scale")]),e("div",{staticClass:"subContent"},[t._v(" The dataset contains more than 10,000 video segments of real-world moving objects and over 1.5 million manually labeled bounding boxes ")])])])]),e("div",{staticClass:"PageContent flex justify-center items-center flex-col"},[e("div",{staticClass:"headtitle text-3xl font-bold p-5"},[t._v(" Comparisons with Existing Benchmarks ")]),e("div",{staticClass:"overview_content w-7/12"},[e("div",[t._v(" We introduce the Massive Multi-discipline Multimodal Understanding and Reasoning (MMMU) benchmark, a novel benchmark meticulously curated to assess the expert-level multimodal understanding capability of foundation models across a broad scope of tasks. Covering subjects across disciplines, including Art, Business, Health & Medicine, Science, Humanities & Social Science, and Tech & Engineering, and over subfields. The detailed subject coverage and statistics are detailed in the figure. The questions in our benchmark were manually collected by a team of college students (including coauthors) from various disciplines and subjects, drawing from online sources, textbooks, and lecture materials. ")]),e("img",{staticClass:"pt-10 pb-2",attrs:{src:"resources/img/figures/question_generation.png",alt:""}}),e("div",{staticClass:"pb-10"},[t._v(" Sampled MMMU examples from each discipline. The questions and images need expert-level knowledge to understand and reason. ")])])])])}],y={},M=y,k=(0,r.Z)(M,C,x,!1,null,"029907c1",null),T=k.exports,P=function(){var t=this,e=t._self._c;return e("div",{staticClass:"Experiments w-full h-full"},[e("div",{staticClass:"bg w-full h-20 flex justify-center items-center font-bold text-4xl"},[t._v(" Experiment Results ")]),t._m(0),e("div",{staticClass:"w-full h-full flex justify-center items-center"},[e("div",{staticClass:"main",staticStyle:{width:"95rem"}},[e("div",{staticClass:"table"},[e("ul",{staticStyle:{"padding-top":"20px"}},[e("li",{staticClass:"one"}),t._l(t.titles,(function(a,s){return e("li",{key:s,staticClass:"two"},[0==s?e("div",[e("P",[t._v("Validation")]),e("P",[t._v("Overall")])],1):e("div",[e("p",[t._v(t._s(a))])])])}))],2),e("ul",{staticStyle:{"padding-bottom":"10px"}},[e("li",{staticClass:"one"}),t._l(t.titles,(function(a,s){return e("li",{key:s,staticClass:"two"},[e("div",0==s?[t._m(1,!0)]:[e("div",{staticClass:"name"},[e("div",{staticClass:"details"},[e("p",[t._v("Tiny")]),e("p",{staticClass:"text-xs",staticStyle:{"padding-top":"5px"}},[t._v(" ( "+t._s(t.details[s].t1)+" ) ")])]),e("div",{staticClass:"details"},[e("p",[t._v("All")]),e("p",{staticClass:"text-xs",staticStyle:{"padding-top":"5px"}},[t._v(" ( "+t._s(t.details[s].t2)+" ) ")])])])])])}))],2),e("div",{staticClass:"table-2"},t._l(t.table1,(function(a){return e("ul",{key:a.prop,staticClass:"row-1"},[e("li",{staticClass:"one"},[e("a",{attrs:{href:a.link,target:"_blank"}},[t._v(t._s(a.prop))])]),t._l(a.data,(function(s,i){return e("li",{key:i,staticClass:"two"},[e("div",0==i?[e("div",{staticClass:"details"},[e("p",[t._v(t._s(a.sorce))])])]:[e("div",{staticClass:"name"},[e("div",{staticClass:"details"},[e("p",[t._v(t._s(a.data[i].t1))])]),e("div",{staticClass:"details"},[e("p",[t._v(t._s(a.data[i].t2))])])])])])}))],2)})),0),e("div",{staticClass:"title-2"},[t._v(" Large Multimodal Models (LMMs): Text + Image as Input ")]),e("div",{staticClass:"table-2"},t._l(t.table2,(function(a){return e("ul",{key:a.prop,staticClass:"row-1"},[e("li",{staticClass:"one"},[e("a",{attrs:{href:a.link,target:"_blank"}},[t._v(t._s(a.prop))])]),t._l(a.data,(function(s,i){return e("li",{key:i,staticClass:"two"},[e("div",0==i?[e("div",{staticClass:"details"},[e("p",[t._v(t._s(a.sorce))])])]:[e("div",{staticClass:"name"},[e("div",{staticClass:"details"},[e("p",[t._v(t._s(a.data[i].t1))])]),e("div",{staticClass:"details"},[e("p",[t._v(t._s(a.data[i].t2))])])])])])}))],2)})),0),e("div",{staticClass:"title-2"},[t._v(" Large Language Models (LLMs): Only Text as Input ")]),e("div",{staticClass:"table-2"},t._l(t.table3,(function(a){return e("ul",{key:a.prop,staticClass:"row-1"},[e("li",{staticClass:"one"},[e("a",{attrs:{href:a.link,target:"_blank"}},[t._v(t._s(a.prop))])]),t._l(a.data,(function(s,i){return e("li",{key:i,staticClass:"two"},[e("div",0==i?[e("div",{staticClass:"details"},[e("p",[t._v(t._s(a.sorce))])])]:[e("div",{staticClass:"name"},[e("div",{staticClass:"details"},[e("p",[t._v(t._s(a.data[i].t1))])]),e("div",{staticClass:"details"},[e("p",[t._v(t._s(a.data[i].t2))])])])])])}))],2)})),0)])])])])},L=[function(){var t=this,e=t._self._c;return e("div",{staticClass:"flex justify-center items-center flex-col"},[e("div",{staticClass:"ExTitle text-3xl font-bold p-5 pt-10"},[t._v("Leaderboard")]),e("div",{staticClass:"leadContent w-7/12 pb-10"},[t._v(" We evaluate various models including LLMs and LMMs. In each type, we consider both closed- and open-source models. Our evaluation is conducted under a zero-shot setting to assess the capability of models to generate accurate answers without fine-tuning or few-shot demonstrations on our benchmark. For all models, we use the default prompt provided by each model for multi-choice or open QA, if available. If models do not provide prompts for task types in MMMU, we conduct prompt engineering on the validation set and use the most effective prompt for the later zero-shot experiment. ")])])},function(){var t=this,e=t._self._c;return e("div",{staticClass:"details"},[e("p",[t._v("-")]),e("p",{staticClass:"text-xs",staticStyle:{"padding-top":"5px"}},[t._v("( 666 )")])])}],S={data(){return{headers:["Validation Overall","Test Overall","WebPathology","Twitter","Youtube","Book"],tableData:[["Random Choice",25.1,25.8,25.2,24.7,25.9,25.5],["Frequent Choice",29.1,27.5,25.8,27.7,30.2,28.4]],titles:["Validation","Test Overall","WebPathology","Twitter","Youtube","Book"],details:[{t1:3445,t2:4667},{t1:3445,t2:4667},{t1:3445,t2:4667},{t1:3445,t2:4667},{t1:3445,t2:4667},{t1:3445,t2:4667}],table1:[{prop:"Random Choice",sorce:"34.1",link:"https://www.baidu.com/",data:[{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4}]},{prop:"Frequent Choice",sorce:"34.1",link:"https://www.baidu.com/",data:[{t1:25.1,t2:29.4},{t1:22.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4}]},{prop:"Expert performance",link:"https://www.baidu.com/",sorce:"34.1",data:[{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:23.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4}]}],table2:[{prop:"OpenFlamingo2-9B",link:"https://www.baidu.com/",sorce:"34.1",data:[{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4}]},{prop:"Kosmos2",link:"https://www.baidu.com/",sorce:"34.1",data:[{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4}]},{prop:"Fuyu-8B",link:"https://www.baidu.com/",sorce:"34.1",data:[{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4}]},{prop:"MiniGPT4-Vicuna-13B",link:"https://www.baidu.com/",sorce:"34.1",data:[{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4}]},{prop:"LLaMA-Adapter2-7B",link:"https://www.baidu.com/",sorce:"34.1",data:[{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4}]},{prop:"Otter",sorce:"34.1",link:"https://www.baidu.com/",data:[{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4}]},{prop:"CogVLM",sorce:"34.1",link:"https://www.baidu.com/",data:[{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4}]},{prop:"BLIP-2 FLAN-T5-XL",sorce:"34.1",link:"https://www.baidu.com/",data:[{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4}]},{prop:"BLIP-2 FLAN-T5-XXL",sorce:"34.1",link:"https://www.baidu.com/",data:[{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4}]},{prop:"Owen-VL-7B",sorce:"34.1",link:"https://www.baidu.com/",data:[{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4}]},{prop:"LLaVA-1.5-7B",sorce:"34.1",link:"https://www.baidu.com/",data:[{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4}]},{prop:"LLaVA-1.5-13B",sorce:"34.1",link:"https://www.baidu.com/",data:[{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4}]},{prop:"InstructBLIP-T5-XL",sorce:"34.1",link:"https://www.baidu.com/",data:[{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4}]},{prop:"InstructBLIP-TS-XXL",sorce:"34.1",link:"https://www.baidu.com/",data:[{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4}]},{prop:"Qwen-VL-PLUS",sorce:"34.1",link:"https://www.baidu.com/",data:[{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4}]},{prop:"Gemini Pro Vision",sorce:"34.1",link:"https://www.baidu.com/",data:[{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4}]},{prop:"GPT-4V",sorce:"34.1",link:"https://www.baidu.com/",data:[{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4}]}],table3:[{prop:" Vicuna-v1.5-13B",sorce:"34.1",link:"https://www.baidu.com/",data:[{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4}]},{prop:"GPT-4 Turbo",sorce:"34.1",link:"https://www.baidu.com/",data:[{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4}]},{prop:"GPT-3.5 Turbo",sorce:"34.1",link:"https://www.baidu.com/",data:[{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4}]},{prop:"Gemini Pro",sorce:"34.1",link:"https://www.baidu.com/",data:[{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4}]},{prop:"ERNIE-Bot 4.0",sorce:"34.1",link:"https://www.baidu.com/",data:[{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4},{t1:25.1,t2:29.4}]}]}}},j=S,O=(0,r.Z)(j,P,L,!1,null,"678061b0",null),A=O.exports,B=function(){var t=this;t._self._c;return t._m(0)},I=[function(){var t=this,e=t._self._c;return e("div",{staticClass:"w-full h-full pt-5"},[e("div",{staticClass:"PageContent flex justify-center items-center flex-col"},[e("div",{staticClass:"headtitle text-3xl font-bold p-5"},[t._v(" Robustness Evaluation of LMMs towards Image Corruption ")]),e("div",{staticClass:"overview_content w-7/12"},[e("div",[t._v(" In the realm of practical pathology, the interpretation of mod404 els significantly influences the subsequent medical decisions and treatment strategies doctors determine. Therefore, the ro406 bustness of these models is crucial for clinical applications. Given that during staining, scanning, or image generation of pathological slides, several factors often affect image quality, including JPEG compression, pixelation, blur(encompasses bubble blur, defocus blur, motion blur), and color variations (such as brightness, saturation, and hue). Inspired by the analysis of encoder-based models’ robustness against pathol413 ogy image corruption [Zheng et al., 2024; Sun et al., 2023a; Zhang et al., 2022], we incorporate these corruptions to ex415 plore the robustness of LMMs against these corruptions on PathMMU dataset. ")]),e("img",{staticClass:"pt-10 pb-2",attrs:{src:"resources/img/figures/corruption_illustration.png",alt:""}}),e("img",{staticClass:"pt-10 pb-2",attrs:{src:"resources/img/figures/attribute.png",alt:""}}),e("div",{staticClass:"pb-10"},[t._v(" Figure 5: Illustration of the model’s performance towards different levels of color-related (brightness, hue, saturation) and image quality-related (pixelation, JPEG compression, bubble effect, motion blur, defocus) corruptions on the PathMMU test-tiny set, where level 0 represents the model’s performance without any corruption. ")])])]),e("div",{staticClass:"PageContent flex justify-center items-center flex-col"},[e("div",{staticClass:"headtitle text-3xl font-bold p-5"},[t._v("Attributes all")]),e("div",{staticClass:"overview_content w-7/12"},[e("img",{staticClass:"pt-10 pb-2",attrs:{src:"resources/img/figures/attribute_all.png",alt:""}}),e("img",{staticClass:"pt-10 pb-2",attrs:{src:"resources/img/figures/attribute_half.png",alt:""}}),e("div",{staticClass:"pb-10"},[t._v(" Figure 5: Illustration of the model’s performance towards different levels of color-related (brightness, hue, saturation) and image quality-related (pixelation, JPEG compression, bubble effect, motion blur, defocus) corruptions on the PathMMU test-tiny set, where level 0 represents the model’s performance without any corruption. ")])])])])}],U={},E=U,F=(0,r.Z)(E,B,I,!1,null,"78e1f644",null),V=F.exports,H=function(){var t=this;t._self._c;return t._m(0)},q=[function(){var t=this,e=t._self._c;return e("div",{staticClass:"FooterPart w-full h-56 flex justify-center items-center text-base",staticStyle:{"background-color":"#fafafa"}},[e("p",[t._v(" This website is website adapted from "),e("a",{attrs:{href:"https://nerfies.github.io/"}},[t._v("Nerfies")]),t._v(" and "),e("a",{attrs:{href:"https://mathvista.github.io/"}},[t._v("MathVista")]),t._v(", licensed under a "),e("a",{attrs:{rel:"license",href:"http://creativecommons.org/licenses/by-sa/4.0/"}},[t._v("Creative Commons Attribution-ShareAlike 4.0 International License")]),t._v(". ")])])}],Z={},G=Z,R=(0,r.Z)(G,H,q,!1,null,null,null),N=R.exports,X={name:"HomeView",components:{TopMenu:_,HomePage:T,ExperimentsPage:A,ThirdPart:V,FooterPart:N},data(){return{}},mounted(){},methods:{}},W=X,J=(0,r.Z)(W,v,f,!1,null,null,null),K=J.exports;n["default"].use(p.ZP);const z=[{path:"/",name:"home",component:K}],Q=new p.ZP({mode:"hash",base:"",routes:z});var Y=Q;n["default"].use(i()),n["default"].config.productionTip=!1,new n["default"]({router:Y,render:function(t){return t(u)}}).$mount("#app")}},e={};function a(s){var i=e[s];if(void 0!==i)return i.exports;var n=e[s]={id:s,loaded:!1,exports:{}};return t[s](n,n.exports,a),n.loaded=!0,n.exports}a.m=t,function(){a.amdO={}}(),function(){var t=[];a.O=function(e,s,i,n){if(!s){var o=1/0;for(d=0;d<t.length;d++){s=t[d][0],i=t[d][1],n=t[d][2];for(var l=!0,r=0;r<s.length;r++)(!1&n||o>=n)&&Object.keys(a.O).every((function(t){return a.O[t](s[r])}))?s.splice(r--,1):(l=!1,n<o&&(o=n));if(l){t.splice(d--,1);var c=i();void 0!==c&&(e=c)}}return e}n=n||0;for(var d=t.length;d>0&&t[d-1][2]>n;d--)t[d]=t[d-1];t[d]=[s,i,n]}}(),function(){a.n=function(t){var e=t&&t.__esModule?function(){return t["default"]}:function(){return t};return a.d(e,{a:e}),e}}(),function(){a.d=function(t,e){for(var s in e)a.o(e,s)&&!a.o(t,s)&&Object.defineProperty(t,s,{enumerable:!0,get:e[s]})}}(),function(){a.g=function(){if("object"===typeof globalThis)return globalThis;try{return this||new Function("return this")()}catch(t){if("object"===typeof window)return window}}()}(),function(){a.o=function(t,e){return Object.prototype.hasOwnProperty.call(t,e)}}(),function(){a.r=function(t){"undefined"!==typeof Symbol&&Symbol.toStringTag&&Object.defineProperty(t,Symbol.toStringTag,{value:"Module"}),Object.defineProperty(t,"__esModule",{value:!0})}}(),function(){a.nmd=function(t){return t.paths=[],t.children||(t.children=[]),t}}(),function(){var t={143:0};a.O.j=function(e){return 0===t[e]};var e=function(e,s){var i,n,o=s[0],l=s[1],r=s[2],c=0;if(o.some((function(e){return 0!==t[e]}))){for(i in l)a.o(l,i)&&(a.m[i]=l[i]);if(r)var d=r(a)}for(e&&e(s);c<o.length;c++)n=o[c],a.o(t,n)&&t[n]&&t[n][0](),t[n]=0;return a.O(d)},s=self["webpackChunkportfolio"]=self["webpackChunkportfolio"]||[];s.forEach(e.bind(null,0)),s.push=e.bind(null,s.push.bind(s))}();var s=a.O(void 0,[998],(function(){return a(6255)}));s=a.O(s)})();
//# sourceMappingURL=app.4202b4f9.js.map