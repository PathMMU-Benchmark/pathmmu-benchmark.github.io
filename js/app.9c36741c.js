(function(){"use strict";var e={8821:function(e,t,n){n.d(t,{Z:function(){return c}});var i=function(){var e=this;e._self._c;return e._m(0)},a=[function(){var e=this,t=e._self._c;return t("div",{staticClass:"TopMenu h-full text-center flex justify-center items-center pt-40 flex-col"},[t("div",{staticClass:"Title"},[e._v(" PathMMU: A Massive Multimodal Expert-Level Benchmark for Understanding "),t("br"),e._v(" and Reasoning in Pathology ")]),t("div",{staticClass:"author pt-10 font-bold"},[e._v("Author Name")]),t("div",{staticClass:"pt-1"},[e._v("Affiliation")]),t("div",[e._v("email@example.com")]),t("div",{staticClass:"overall w-7/12 pt-5"},[t("img",{attrs:{src:"resources/img/figures/overall.png",alt:""}}),t("div",{staticClass:"pt-3 pb-3"},[e._v(" Figure 1: An overview of the PathMMU benchmark: PathMMU is constructed using a diverse range of rich data sources. It comprises expert-level, multimodal, multi-choice questions in pathology, collaboratively crafted by AI and human pathology experts. Notably, even the most advanced Large Multimodal Models substantially underperform when benchmarked against human experts on the PathMMU. ")])]),t("div",{staticClass:"Intro pt-10 w-7/12"},[t("div",{staticClass:"head font-bold pb-5"},[e._v("Introduction")]),t("div",{staticClass:"content text-center pb-10"},[e._v(" The Pathology Multimodal Multitask Unsupervised Benchmark (MMMU) dataset is a large-scale, multimodal, multitask dataset for understanding and reasoning in pathology. The dataset is designed to be comprehensive, diverse, and challenging, and is intended to serve as a benchmark for research in multimodal pathology understanding and reasoning. ")])])])}],o={data(){return{publicUrl:"./public/",topMenu:[{content:"HOME",id:0},{content:"ABOUT",id:2},{content:"CONCACT",id:3}]}}},s=o,r=n(1001),l=(0,r.Z)(s,i,a,!1,null,"22445a9a",null),c=l.exports},4524:function(e,t,n){var i=n(4720),a=n.n(i),o=n(144),s=function(){var e=this,t=e._self._c;return t("div",{attrs:{id:"app"}},[t("router-view")],1)},r=[],l=n(1001),c={},u=(0,l.Z)(c,s,r,!1,null,null,null),d=u.exports,f=n(8345),v=function(){var e=this,t=e._self._c;return t("div",{staticClass:"HomePage h-full w-full text-lg"},[t("TopMenu",{ref:"TopMenu"}),t("HomePage"),t("ExperimentsPage")],1)},m=[],h=n(8821),p=function(){var e=this;e._self._c;return e._m(0)},g=[function(){var e=this,t=e._self._c;return t("div",{staticClass:"HomePage w-full h-full"},[t("div",{staticClass:"bg w-full h-20 flex justify-center items-center font-bold text-4xl"},[e._v(" PATHMMU BENCHMARK ")]),t("div",{staticClass:"PageContent flex justify-center items-center flex-col"},[t("div",{staticClass:"headtitle text-3xl font-bold p-5"},[e._v("Overview")]),t("div",{staticClass:"overview_content w-7/12"},[t("div",[e._v(" We introduce the Massive Multi-discipline Multimodal Understanding and Reasoning (MMMU) benchmark, a novel benchmark meticulously curated to assess the expert-level multimodal understanding capability of foundation models across a broad scope of tasks. Covering subjects across disciplines, including Art, Business, Health & Medicine, Science, Humanities & Social Science, and Tech & Engineering, and over subfields. The detailed subject coverage and statistics are detailed in the figure. The questions in our benchmark were manually collected by a team of college students (including coauthors) from various disciplines and subjects, drawing from online sources, textbooks, and lecture materials. ")]),t("img",{staticClass:"pt-10 pb-10",attrs:{src:"resources/img/figures/overall.png",alt:""}}),t("div")])]),t("div",{staticClass:"w-7/12 pt-10",staticStyle:{margin:"0 auto"}},[t("div",{staticClass:"FeatureTitle"},[e._v("Key Features")]),t("div",{staticClass:"FeatureContent grid grid-cols-3 gap-3"},[t("div",[t("div",{staticClass:"SubTitle"},[e._v("Large-Scale")]),t("div",{staticClass:"subContent"},[e._v(" The dataset contains more than 10,000 video segments of real-world moving objects and over 1.5 million manually labeled bounding boxes ")])]),t("div",[t("div",{staticClass:"SubTitle"},[e._v("Large-Scale")]),t("div",{staticClass:"subContent"},[e._v(" The dataset contains more than 10,000 video segments of real-world moving objects and over 1.5 million manually labeled bounding boxes ")])]),t("div",[t("div",{staticClass:"SubTitle"},[e._v("Large-Scale")]),t("div",{staticClass:"subContent"},[e._v(" The dataset contains more than 10,000 video segments of real-world moving objects and over 1.5 million manually labeled bounding boxes ")])]),t("div",[t("div",{staticClass:"SubTitle"},[e._v("Large-Scale")]),t("div",{staticClass:"subContent"},[e._v(" The dataset contains more than 10,000 video segments of real-world moving objects and over 1.5 million manually labeled bounding boxes ")])]),t("div",[t("div",{staticClass:"SubTitle"},[e._v("Large-Scale")]),t("div",{staticClass:"subContent"},[e._v(" The dataset contains more than 10,000 video segments of real-world moving objects and over 1.5 million manually labeled bounding boxes ")])]),t("div",[t("div",{staticClass:"SubTitle"},[e._v("Large-Scale")]),t("div",{staticClass:"subContent"},[e._v(" The dataset contains more than 10,000 video segments of real-world moving objects and over 1.5 million manually labeled bounding boxes ")])])])]),t("div",{staticClass:"PageContent flex justify-center items-center flex-col"},[t("div",{staticClass:"headtitle text-3xl font-bold p-5"},[e._v(" Comparisons with Existing Benchmarks ")]),t("div",{staticClass:"overview_content w-7/12"},[t("div",[e._v(" We introduce the Massive Multi-discipline Multimodal Understanding and Reasoning (MMMU) benchmark, a novel benchmark meticulously curated to assess the expert-level multimodal understanding capability of foundation models across a broad scope of tasks. Covering subjects across disciplines, including Art, Business, Health & Medicine, Science, Humanities & Social Science, and Tech & Engineering, and over subfields. The detailed subject coverage and statistics are detailed in the figure. The questions in our benchmark were manually collected by a team of college students (including coauthors) from various disciplines and subjects, drawing from online sources, textbooks, and lecture materials. ")]),t("img",{staticClass:"pt-10 pb-2",attrs:{src:"resources/img/figures/question_generation.png",alt:""}}),t("div",{staticClass:"pb-10"},[e._v(" Sampled MMMU examples from each discipline. The questions and images need expert-level knowledge to understand and reason. ")])])])])}],b={},C=b,y=(0,l.Z)(C,p,g,!1,null,"029907c1",null),_=y.exports,x=function(){var e=this;e._self._c;return e._m(0)},w=[function(){var e=this,t=e._self._c;return t("div",{staticClass:"Experiments w-full h-full"},[t("div",{staticClass:"bg w-full h-20 flex justify-center items-center font-bold text-4xl"},[e._v(" Experiment Results ")]),t("div",{staticClass:"flex justify-center items-center flex-col"},[t("div",{staticClass:"ExTitle text-3xl font-bold p-5"},[e._v("Leaderboard")]),t("div",{staticClass:"leadContent w-7/12 pb-10"},[e._v(" We evaluate various models including LLMs and LMMs. In each type, we consider both closed- and open-source models. Our evaluation is conducted under a zero-shot setting to assess the capability of models to generate accurate answers without fine-tuning or few-shot demonstrations on our benchmark. For all models, we use the default prompt provided by each model for multi-choice or open QA, if available. If models do not provide prompts for task types in MMMU, we conduct prompt engineering on the validation set and use the most effective prompt for the later zero-shot experiment. ")])]),t("div",{staticClass:"w-7/12",staticStyle:{margin:"0 auto"}})])}],M={data(){return{headers:["Validation Overall","Test Overall","WebPathology","Twitter","Youtube","Book"],tableData:[["Random Choice",25.1,25.8,25.2,24.7,25.9,25.5],["Frequent Choice",29.1,27.5,25.8,27.7,30.2,28.4]]}}},T=M,k=(0,l.Z)(T,x,w,!1,null,"7317bc82",null),S=k.exports,j={name:"HomeView",components:{TopMenu:h.Z,HomePage:_,ExperimentsPage:S},data(){return{}},mounted(){},methods:{}},P=j,A=(0,l.Z)(P,v,m,!1,null,null,null),O=A.exports;o["default"].use(f.ZP);const E=[{path:"/",name:"home",component:O},{path:"/details/:id",name:"Details",component:function(){return n.e(807).then(n.bind(n,6807))}},{path:"/animation",name:"Animation",component:function(){return n.e(366).then(n.bind(n,1366))}},{path:"/advertise",name:"Advertise",component:function(){return n.e(22).then(n.bind(n,6022))}},{path:"/clip",name:"Clip",component:function(){return n.e(138).then(n.bind(n,9138))}},{path:"/modeling",name:"Modeling",component:function(){return n.e(59).then(n.bind(n,4059))}},{path:"/plane",name:"Plane",component:function(){return n.e(49).then(n.bind(n,3049))}},{path:"/article",name:"Article",component:function(){return n.e(36).then(n.bind(n,7036))}}],L=new f.ZP({mode:"hash",base:"",routes:E});var U=L;o["default"].use(a()),o["default"].config.productionTip=!1,new o["default"]({router:U,render:function(e){return e(d)}}).$mount("#app")}},t={};function n(i){var a=t[i];if(void 0!==a)return a.exports;var o=t[i]={id:i,loaded:!1,exports:{}};return e[i](o,o.exports,n),o.loaded=!0,o.exports}n.m=e,function(){n.amdO={}}(),function(){var e=[];n.O=function(t,i,a,o){if(!i){var s=1/0;for(u=0;u<e.length;u++){i=e[u][0],a=e[u][1],o=e[u][2];for(var r=!0,l=0;l<i.length;l++)(!1&o||s>=o)&&Object.keys(n.O).every((function(e){return n.O[e](i[l])}))?i.splice(l--,1):(r=!1,o<s&&(s=o));if(r){e.splice(u--,1);var c=a();void 0!==c&&(t=c)}}return t}o=o||0;for(var u=e.length;u>0&&e[u-1][2]>o;u--)e[u]=e[u-1];e[u]=[i,a,o]}}(),function(){n.n=function(e){var t=e&&e.__esModule?function(){return e["default"]}:function(){return e};return n.d(t,{a:t}),t}}(),function(){n.d=function(e,t){for(var i in t)n.o(t,i)&&!n.o(e,i)&&Object.defineProperty(e,i,{enumerable:!0,get:t[i]})}}(),function(){n.f={},n.e=function(e){return Promise.all(Object.keys(n.f).reduce((function(t,i){return n.f[i](e,t),t}),[]))}}(),function(){n.u=function(e){return"js/"+e+"."+{22:"e15c3536",36:"7bf4f67e",49:"fcf3b250",59:"3c155ae8",138:"403371c8",366:"7edbc186",807:"3aae4451"}[e]+".js"}}(),function(){n.miniCssF=function(e){return"css/"+e+"."+{22:"9165ff6e",36:"c035083b",49:"24dc91c5",59:"6021f2be",138:"4ad052b9",366:"bcb150ed",807:"22bbdfd1"}[e]+".css"}}(),function(){n.g=function(){if("object"===typeof globalThis)return globalThis;try{return this||new Function("return this")()}catch(e){if("object"===typeof window)return window}}()}(),function(){n.o=function(e,t){return Object.prototype.hasOwnProperty.call(e,t)}}(),function(){var e={},t="portfolio:";n.l=function(i,a,o,s){if(e[i])e[i].push(a);else{var r,l;if(void 0!==o)for(var c=document.getElementsByTagName("script"),u=0;u<c.length;u++){var d=c[u];if(d.getAttribute("src")==i||d.getAttribute("data-webpack")==t+o){r=d;break}}r||(l=!0,r=document.createElement("script"),r.charset="utf-8",r.timeout=120,n.nc&&r.setAttribute("nonce",n.nc),r.setAttribute("data-webpack",t+o),r.src=i),e[i]=[a];var f=function(t,n){r.onerror=r.onload=null,clearTimeout(v);var a=e[i];if(delete e[i],r.parentNode&&r.parentNode.removeChild(r),a&&a.forEach((function(e){return e(n)})),t)return t(n)},v=setTimeout(f.bind(null,void 0,{type:"timeout",target:r}),12e4);r.onerror=f.bind(null,r.onerror),r.onload=f.bind(null,r.onload),l&&document.head.appendChild(r)}}}(),function(){n.r=function(e){"undefined"!==typeof Symbol&&Symbol.toStringTag&&Object.defineProperty(e,Symbol.toStringTag,{value:"Module"}),Object.defineProperty(e,"__esModule",{value:!0})}}(),function(){n.nmd=function(e){return e.paths=[],e.children||(e.children=[]),e}}(),function(){n.p=""}(),function(){if("undefined"!==typeof document){var e=function(e,t,n,i,a){var o=document.createElement("link");o.rel="stylesheet",o.type="text/css";var s=function(n){if(o.onerror=o.onload=null,"load"===n.type)i();else{var s=n&&("load"===n.type?"missing":n.type),r=n&&n.target&&n.target.href||t,l=new Error("Loading CSS chunk "+e+" failed.\n("+r+")");l.code="CSS_CHUNK_LOAD_FAILED",l.type=s,l.request=r,o.parentNode&&o.parentNode.removeChild(o),a(l)}};return o.onerror=o.onload=s,o.href=t,n?n.parentNode.insertBefore(o,n.nextSibling):document.head.appendChild(o),o},t=function(e,t){for(var n=document.getElementsByTagName("link"),i=0;i<n.length;i++){var a=n[i],o=a.getAttribute("data-href")||a.getAttribute("href");if("stylesheet"===a.rel&&(o===e||o===t))return a}var s=document.getElementsByTagName("style");for(i=0;i<s.length;i++){a=s[i],o=a.getAttribute("data-href");if(o===e||o===t)return a}},i=function(i){return new Promise((function(a,o){var s=n.miniCssF(i),r=n.p+s;if(t(s,r))return a();e(i,r,null,a,o)}))},a={143:0};n.f.miniCss=function(e,t){var n={22:1,36:1,49:1,59:1,138:1,366:1,807:1};a[e]?t.push(a[e]):0!==a[e]&&n[e]&&t.push(a[e]=i(e).then((function(){a[e]=0}),(function(t){throw delete a[e],t})))}}}(),function(){var e={143:0};n.f.j=function(t,i){var a=n.o(e,t)?e[t]:void 0;if(0!==a)if(a)i.push(a[2]);else{var o=new Promise((function(n,i){a=e[t]=[n,i]}));i.push(a[2]=o);var s=n.p+n.u(t),r=new Error,l=function(i){if(n.o(e,t)&&(a=e[t],0!==a&&(e[t]=void 0),a)){var o=i&&("load"===i.type?"missing":i.type),s=i&&i.target&&i.target.src;r.message="Loading chunk "+t+" failed.\n("+o+": "+s+")",r.name="ChunkLoadError",r.type=o,r.request=s,a[1](r)}};n.l(s,l,"chunk-"+t,t)}},n.O.j=function(t){return 0===e[t]};var t=function(t,i){var a,o,s=i[0],r=i[1],l=i[2],c=0;if(s.some((function(t){return 0!==e[t]}))){for(a in r)n.o(r,a)&&(n.m[a]=r[a]);if(l)var u=l(n)}for(t&&t(i);c<s.length;c++)o=s[c],n.o(e,o)&&e[o]&&e[o][0](),e[o]=0;return n.O(u)},i=self["webpackChunkportfolio"]=self["webpackChunkportfolio"]||[];i.forEach(t.bind(null,0)),i.push=t.bind(null,i.push.bind(i))}();var i=n.O(void 0,[998],(function(){return n(4524)}));i=n.O(i)})();
//# sourceMappingURL=app.9c36741c.js.map